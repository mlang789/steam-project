# Evaluation Suite Usage Guide

This directory contains three Python scripts located in `src/evaluation/` for evaluating the quality, diversity, and realism of reviews generated by your models.

## Prerequisites

All scripts require a Python environment with the following libraries:

```bash
pip install pandas numpy requests tqdm scikit-learn sentence-transformers tabulate
```

> **Note:** `sentence-transformers` is large but necessary for semantic analysis (SBERT).

---

## 1. Quality Evaluation & LLM-as-a-Judge

**Script:** `08_evaluate_quality.py`

This script generates prompts for an "AI Judge" (GPT-4/Claude) to detect hallucinations, spoilers, or structural errors. It also calculates sentiment accuracy metrics (SBERT vs Judge).

**Syntax:**
```bash
python src/evaluation/08_evaluate_quality.py [TASK]
```

### Available Tasks

| Argument | Description | Outputs (in `evaluation/`) |
| :--- | :--- | :--- |
| `all` | Executes all prompt generations + SBERT preparation. | All files below. |
| `hallucination` | Prompts to detect invented facts. | `prompts/batch_hallucination_*.txt` |
| `structure` | Prompts to verify the "2 Positive / 1 Negative" rule. | `prompts/batch_structure_*.txt` |
| `spoiler` | Prompts to detect narrative spoilers. | `prompts/batch_spoiler_*.txt` |
| `sentiment` | Prompts to verify rating/text alignment. | `prompts/batch_sentiment_naive.txt` |
| `sbert_prep` | Prepares sample for SBERT evaluation (see below). | `prompts/prompt_judge_sbert_300.txt`<br>`csv/sbert_subset_300_stratified.csv` |
| `sbert_eval` | Compares SBERT predictions vs Judge (requires manual step). | `csv/sbert_evaluation_results_300.csv` |

### Specific Workflow: SBERT Evaluation
1.  Run `python src/evaluation/08_evaluate_quality.py sbert_prep`.
2.  Copy the content of `evaluation/prompts/prompt_judge_sbert_300.txt` into ChatGPT/Claude.
3.  Retrieve **only** the JSON response and save it as `evaluation/csv/judge_labels_300.json`.
4.  Run `python src/evaluation/08_evaluate_quality.py sbert_eval`.

---

## 2. Diversity Evaluation

**Script:** `09_evaluate_diversity.py`

Measures if the model "goes in circles." It analyzes vocabulary richness (n-grams) and semantic redundancy between generated reviews.

**Syntax:**
```bash
python src/evaluation/09_evaluate_diversity.py --input [CSV_FILE] --save [OPTIONS]
```

### Key Arguments
*   `--inter-sim`: Activates SBERT semantic analysis (recommended to detect if reviews all say the same thing).
*   `--save`: Adds results to the report `results/results_diversity.md`.
*   `--prefix "Title"`: Experiment name in the report.

**Complete example:**
```bash
python src/evaluation/09_evaluate_diversity.py \
  --input reports/genai_inputs/prompt_batch_filled.csv \
  --inter-sim \
  --save \
  --prefix "Naive vs Engineered Comparison"
```

---

## 3. Realism & Plagiarism Evaluation

**Script:** `10_evaluate_realism.py`

Compares your generated reviews with a base of "real" Steam reviews to see if they "sound authentic" (semantic proximity) and verify they are not pure copy-paste (plagiarism).

**Syntax:**
```bash
python src/evaluation/10_evaluate_realism.py --gen [AI_CSV] --real [REAL_CSV] --save [OPTIONS]
```

### Key Arguments
*   `--gen`: Generated reviews file.
*   `--real`: Real reviews file (Ground Truth).
*   `--max-real 2000`: Limits the number of real reviews used (recommended to speed up computation).
*   `--save`: Adds results to the report `results/results_realism.md`.

**Complete example:**
```bash
python src/evaluation/10_evaluate_realism.py \
  --gen reports/genai_inputs/prompt_batch_filled.csv \
  --real data/raw/reviews_raw_train.csv \
  --max-real 2000 \
  --save \
  --prefix "Realism Test V1"
```

---

---

## Output Architecture

Executing these scripts automatically populates the following directory structure:

```text
PROJECT_ROOT/
├── evaluation/                 # Quality script outputs
│   ├── csv/
│   │   ├── sbert_subset_300_stratified.csv
│   │   ├── judge_labels_300.json (MANUAL FILE)
│   │   └── sbert_evaluation_results_300.csv
│   └── prompts/
│       ├── batch_hallucination_*.txt
│       ├── prompt_judge_sbert_300.txt
│       └── ...
│
└── reports/                    # Input files and Markdown reports
    ├── genai_inputs/           # Input CSV files
    │   └── prompt_batch_filled.csv
    └── results/                # Cumulative reports (Diversity & Realism)
        ├── results_diversity.md
        └── results_realism.md
```