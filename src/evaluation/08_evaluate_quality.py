import sys
import argparse
import json
import re
import time
from pathlib import Path

import requests
import pandas as pd
from tqdm import tqdm

# Importations conditionnelles pour les métriques SBERT
try:
    from sentence_transformers import SentenceTransformer, util
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
except ImportError:
    pass

# Configuration du chemin d'accès pour importer src.config
# Le script est dans src/evaluation, on remonte de 2 niveaux pour atteindre la racine src
sys.path.append(str(Path(__file__).resolve().parents[2]))
from src.config import GENAI_INPUTS_DIR, PROJECT_ROOT

# Définition des chemins d'entrée
INPUT_FILE = GENAI_INPUTS_DIR / "prompt_batch_filled.csv"

# Définition des dossiers de sortie spécifiques à la racine (evaluation/prompts et evaluation/csv)
ROOT_EVAL_OUTPUT = PROJECT_ROOT / "evaluation"
PROMPTS_DIR = ROOT_EVAL_OUTPUT / "prompts"
CSV_DIR = ROOT_EVAL_OUTPUT / "csv"

# Création automatique des dossiers de sortie s'ils n'existent pas
for d in [PROMPTS_DIR, CSV_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# Constantes globales pour l'API Steam et l'échantillonnage
TARGET_APP_IDS = [570, 730, 440, 1091500, 1245620]
N_SAMPLES = 50

def clean_text(text):
    """
    Nettoyage élémentaire du texte pour éviter les erreurs de formatage.
    """
    text = re.sub(r'\s+', ' ', str(text).strip())
    return text.replace('"', "'")

def get_game_details(app_ids):
    """
    Récupération des métadonnées (titre et description) via l'api steam.
    """
    print(f"--- Récupération des informations pour {len(app_ids)} jeux ---")
    game_data = {}
    with requests.Session() as s:
        for app_id in tqdm(app_ids):
            try:
                url = "https://store.steampowered.com/api/appdetails"
                params = {"appids": app_id, "l": "english"}
                resp = s.get(url, params=params, timeout=10)
                data = resp.json()
                if data[str(app_id)]["success"]:
                    item = data[str(app_id)]["data"]
                    raw_desc = item.get("short_description", "")
                    clean_desc = re.sub(re.compile('<.*?>'), '', raw_desc)
                    
                    game_data[app_id] = {
                        "title": item.get("name", ""),
                        "description": clean_desc
                    }
            except Exception:
                pass
            time.sleep(0.5)
    return game_data

def format_batch_list(reviews_list):
    """
    Formate une liste de reviews avec un identifiant numérique.
    """
    formatted = []
    for idx, text in enumerate(reviews_list):
        formatted.append(f'[ID: {idx+1}] "{clean_text(text)}"')
    return "\n".join(formatted)

# -------------------------------------------------------------------------
# Tâche 1 : Génération de prompts pour la détection d'hallucinations
# -------------------------------------------------------------------------
def generate_hallucination_prompts(df, game_details):
    methods = ["naive", "engineered", "finetuned"]
    print("\n--- Génération des fichiers pour détection d'hallucinations ---")
    
    for method in methods:
        method_df = df[df["method"] == method]
        if method_df.empty: continue

        out_path = PROMPTS_DIR / f"batch_hallucination_{method}.txt"
        
        with open(out_path, "w", encoding="utf-8") as f:
            for app_id in TARGET_APP_IDS:
                if app_id not in game_details: continue
                
                reviews_series = method_df[method_df["app_id"] == app_id]["generated_text"].dropna()
                if len(reviews_series) > N_SAMPLES:
                    reviews_series = reviews_series.sample(n=N_SAMPLES, random_state=42)
                
                reviews_block = format_batch_list(reviews_series.tolist())
                if not reviews_block: continue

                game_title = game_details[app_id]["title"]
                game_desc = game_details[app_id]["description"]

                prompt = f"""
================================================================================
TASK: BULK HALLUCINATION DETECTION
GAME: {game_title}
OFFICIAL DESCRIPTION: {game_desc}
================================================================================

You are an expert video game Fact-Checker. 
Below is a list of reviews generated by an AI for the game "{game_title}".

YOUR MISSION:
1. Scan EVERY review in the list below.
2. Check if the review describes the correct game (gameplay, genre, characters).
3. Detect "Hallucinations":
   - Describing a different game.
   - Inventing characters or mechanics that do not exist in {game_title}.
4. Count how many reviews are hallucinated.

LIST OF REVIEWS TO CHECK:
-------------------------
{reviews_block}
-------------------------

OUTPUT FORMAT (JSON ONLY):
{{
  "game": "{game_title}",
  "total_reviews_checked": {len(reviews_series)},
  "hallucination_count": <INTEGER>,
  "hallucination_percentage": "<FLOAT>%",
  "hallucinated_ids": [<LIST OF INTEGER IDs>],
  "analysis": "Summary of errors found."
}}
"""
                f.write(prompt)
                f.write("\n\n" + "#" * 80 + "\n\n")
        
        print(f"Fichier créé : {out_path.name}")

# -------------------------------------------------------------------------
# Tâche 2 : Génération de prompts pour la vérification de structure
# -------------------------------------------------------------------------
def generate_structure_prompts(df, game_details):
    methods = ["naive", "engineered", "finetuned"]
    print("\n--- Génération des fichiers pour vérification de structure ---")
    
    for method in methods:
        method_df = df[df["method"] == method]
        if method_df.empty: continue

        out_path = PROMPTS_DIR / f"batch_structure_{method}.txt"
        
        with open(out_path, "w", encoding="utf-8") as f:
            for app_id in TARGET_APP_IDS:
                if app_id not in game_details: continue
                
                reviews_series = method_df[method_df["app_id"] == app_id]["generated_text"].dropna()
                if len(reviews_series) > N_SAMPLES:
                    reviews_series = reviews_series.sample(n=N_SAMPLES, random_state=42)
                
                reviews_block = format_batch_list(reviews_series.tolist())
                if not reviews_block: continue

                game_title = game_details[app_id]["title"]

                prompt = f"""
================================================================================
TASK: BULK STRUCTURE COMPLIANCE CHECK
GAME: {game_title}
CONSTRAINT: Reviews must contain exactly 2 Positive points and 1 Negative point.
================================================================================

You are an expert AI Output Auditor. 
Below is a list of reviews generated by an AI for the game "{game_title}".
The AI was instructed to follow a strict format: **Provide exactly two positive arguments and one negative argument.**

YOUR MISSION:
1. Scan EVERY review.
2. Analyze the sentiment structure.
3. Determine if the review strictly follows the "2 Positives / 1 Negative" rule.
   - FAIL: Different balance (e.g., 1 pro/1 con, 3 pros/0 cons).

LIST OF REVIEWS TO CHECK:
-------------------------
{reviews_block}
-------------------------

OUTPUT FORMAT (JSON ONLY):
{{
  "game": "{game_title}",
  "total_reviews_checked": {len(reviews_series)},
  "structure_failure_count": <INTEGER>,
  "structure_failure_percentage": "<FLOAT>%",
  "failed_ids": [<LIST OF INTEGER IDs>],
  "analysis": "Summary of structural errors."
}}
"""
                f.write(prompt)
                f.write("\n\n" + "#" * 80 + "\n\n")
        
        print(f"Fichier créé : {out_path.name}")

# -------------------------------------------------------------------------
# Tâche 3 : Génération de prompts pour la détection de spoilers
# -------------------------------------------------------------------------
def generate_spoiler_prompts(df, game_details):
    methods = ["naive", "engineered", "finetuned"]
    print("\n--- Génération des fichiers pour détection de spoilers ---")
    
    for method in methods:
        method_df = df[df["method"] == method]
        if method_df.empty: continue

        out_path = PROMPTS_DIR / f"batch_spoiler_{method}.txt"
        
        with open(out_path, "w", encoding="utf-8") as f:
            for app_id in TARGET_APP_IDS:
                if app_id not in game_details: continue
                
                reviews_series = method_df[method_df["app_id"] == app_id]["generated_text"].dropna()
                if len(reviews_series) > N_SAMPLES:
                    reviews_series = reviews_series.sample(n=N_SAMPLES, random_state=42)
                
                reviews_block = format_batch_list(reviews_series.tolist())
                if not reviews_block: continue

                game_title = game_details[app_id]["title"]
                game_desc = game_details[app_id]["description"]

                prompt = f"""
================================================================================
TASK: BULK SPOILER DETECTION
GAME: {game_title}
OFFICIAL DESCRIPTION: {game_desc}
CONSTRAINT: Reviews must NOT contain spoilers.
================================================================================

You are an expert Video Game Content Moderator. 
Below is a list of reviews generated by an AI for the game "{game_title}".
The AI was strictly instructed: **"Do not include spoilers."**

YOUR MISSION:
1. Scan EVERY review.
2. Detect any plot spoilers or significant narrative reveals (ending, deaths, twists).
3. Count how many reviews FAILED the "No Spoiler" constraint.

LIST OF REVIEWS TO CHECK:
-------------------------
{reviews_block}
-------------------------

OUTPUT FORMAT (JSON ONLY):
{{
  "game": "{game_title}",
  "total_reviews_checked": {len(reviews_series)},
  "spoiler_count": <INTEGER>,
  "spoiler_percentage": "<FLOAT>%",
  "spoiler_ids": [<LIST OF INTEGER IDs>],
  "analysis": "Summary of spoilers found."
}}
"""
                f.write(prompt)
                f.write("\n\n" + "#" * 80 + "\n\n")
        
        print(f"Fichier créé : {out_path.name}")

# -------------------------------------------------------------------------
# Tâche 4 : Vérification de l'alignement du sentiment (Méthode naive)
# -------------------------------------------------------------------------
def generate_sentiment_prompts(df, game_details):
    print("\n--- Génération du fichier pour alignement de sentiment (naive) ---")
    
    # Filtre spécifique : méthode naive uniquement
    naive_df = df[df["method"] == "naive"].copy()
    if naive_df.empty:
        print("Aucune donnée trouvée pour la méthode 'naive'.")
        return

    out_path = PROMPTS_DIR / "batch_sentiment_naive.txt"
    samples_per_rating = 25

    with open(out_path, "w", encoding="utf-8") as f:
        for app_id in TARGET_APP_IDS:
            if app_id not in game_details: continue
            
            game_title = game_details[app_id]["title"]
            
            # Traitement pour rating 3 (Négatif) et 9 (Positif)
            for rating, sentiment in [(3, "NEGATIVE"), (9, "POSITIVE")]:
                series = naive_df[(naive_df["app_id"] == app_id) & (naive_df["rating"] == rating)]["generated_text"].dropna()
                
                if len(series) > samples_per_rating:
                    series = series.sample(n=samples_per_rating, random_state=42)
                
                reviews_list = series.tolist()
                if not reviews_list: continue
                
                reviews_block = format_batch_list(reviews_list)
                
                prompt = f"""
================================================================================
TASK: BATCH SENTIMENT ALIGNMENT CHECK
GAME: {game_title}
TARGET RATING: {rating}/10
EXPECTED SENTIMENT: {sentiment}
================================================================================

You are an expert AI Output Auditor.
Below is a list of reviews generated based on a specific input rating of **{rating}/10**.

YOUR MISSION:
1. Scan EVERY review.
2. Check if the review matches the **{sentiment}** expectation.
   - If Target is NEGATIVE (3): Critical, flaws, bugs.
   - If Target is POSITIVE (9): Praising, recommending.
3. Count how many reviews **SUCCESSFULLY** matched the sentiment.

LIST OF REVIEWS TO CHECK:
-------------------------
{reviews_block}
-------------------------

OUTPUT FORMAT (JSON ONLY):
{{
  "game": "{game_title}",
  "target_rating": {rating},
  "expected_sentiment": "{sentiment}",
  "total_reviews_checked": {len(reviews_list)},
  "correct_sentiment_count": <INTEGER>,
  "accuracy_percentage": "<FLOAT>%",
  "failed_ids": [<LIST OF INTEGER IDs>],
  "analysis": "Brief summary."
}}
"""
                f.write(prompt)
                f.write("\n\n" + "-" * 40 + "\n\n")
            
            f.write("\n\n" + "#" * 80 + "\n\n")

    print(f"Fichier créé : {out_path.name}")

# -------------------------------------------------------------------------
# Tâche 5 : Préparation du subset SBERT et du prompt juge
# -------------------------------------------------------------------------
def prepare_sbert_subset(df):
    print("\n--- Préparation de l'échantillon SBERT et du prompt pour le juge ---")
    
    # Sortie dans les dossiers spécifiques à la racine
    output_subset = CSV_DIR / "sbert_subset_300_stratified.csv"
    output_prompt = PROMPTS_DIR / "prompt_judge_sbert_300.txt"
    
    samples_per_method = 100
    methods = ["naive", "engineered", "finetuned"]
    
    sampled_dfs = []
    
    for method in methods:
        method_df = df[df["method"] == method].dropna(subset=["generated_text"])
        count = min(len(method_df), samples_per_method)
        if count > 0:
            subset = method_df.sample(n=count, random_state=42)
            sampled_dfs.append(subset)
            print(f"   - {method}: {count} reviews sélectionnées.")
        else:
            print(f"   Attention: aucune review pour {method}.")

    if not sampled_dfs:
        print("Aucune donnée à traiter.")
        return

    final_subset = pd.concat(sampled_dfs).reset_index(drop=True)
    final_subset["temp_id"] = final_subset.index
    
    final_subset.to_csv(output_subset, index=False)
    print(f"Subset sauvegardé : {output_subset}")
    
    # Génération du prompt
    reviews_json_str = []
    for _, row in final_subset.iterrows():
        short_text = clean_text(row["generated_text"][:300])
        reviews_json_str.append(f'{{ "id": {row["temp_id"]}, "text": "{short_text}..." }}')

    reviews_block = ",\n".join(reviews_json_str)
    
    prompt = f"""
You are a Sentiment Analysis Engine.
Here is a list of {len(final_subset)} video game reviews.

TASK:
Classify EACH review as either "POSITIVE" or "NEGATIVE".

DATA:
[
{reviews_block}
]

OUTPUT FORMAT (Strict JSON list of objects):
[
  {{ "id": 0, "sentiment": "POSITIVE" }},
  {{ "id": 1, "sentiment": "NEGATIVE" }},
  ...
]
Do not write anything else. Just the JSON.
"""
    with open(output_prompt, "w", encoding="utf-8") as f:
        f.write(prompt)
    
    print(f"Prompt pour le juge généré : {output_prompt}")
    print("Action requise : Faites traiter ce prompt par un llm et sauvegardez le json dans 'evaluation/csv/judge_labels_300.json'.")

# -------------------------------------------------------------------------
# Tâche 6 : Évaluation SBERT (Comparaison Prédiction vs Juge)
# -------------------------------------------------------------------------
def evaluate_sbert_accuracy():
    print("\n--- Évaluation de la précision avec SBERT ---")
    
    # Chemins basés sur le dossier CSV défini plus haut
    subset_file = CSV_DIR / "sbert_subset_300_stratified.csv"
    labels_file = CSV_DIR / "judge_labels_300.json"
    results_file = CSV_DIR / "sbert_evaluation_results_300.csv"
    
    if not subset_file.exists():
        print("Erreur: fichier subset manquant. Lancez l'étape 'sbert_prep' d'abord.")
        return
    if not labels_file.exists():
        print(f"Erreur: fichier {labels_file.name} manquant. Vérifiez qu'il est bien dans evaluation/csv/")
        return

    df = pd.read_csv(subset_file)
    
    try:
        with open(labels_file, "r", encoding="utf-8") as f:
            ground_truth_data = json.load(f)
    except json.JSONDecodeError:
        print("Erreur: le fichier json semble mal formé.")
        return
    
    ground_truth_map = {item['id']: item['sentiment'] for item in ground_truth_data}
    df["ground_truth"] = df["temp_id"].map(ground_truth_map)
    df = df.dropna(subset=["ground_truth"])
    
    print("Chargement du modèle SBERT...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    anchor_pos = model.encode("A very good, fun and positive video game review.", convert_to_tensor=True)
    anchor_neg = model.encode("A bad, boring and negative video game review.", convert_to_tensor=True)
    
    def predict_sentiment(text):
        emb = model.encode(text, convert_to_tensor=True)
        score_pos = util.cos_sim(emb, anchor_pos).item()
        score_neg = util.cos_sim(emb, anchor_neg).item()
        return "POSITIVE" if score_pos > score_neg else "NEGATIVE"

    print("Calcul des prédictions...")
    df["sbert_prediction"] = df["generated_text"].apply(predict_sentiment)
    
    acc_global = accuracy_score(df["ground_truth"], df["sbert_prediction"])
    print(f"\nPrécision globale : {acc_global:.2%}")
    print(classification_report(df["ground_truth"], df["sbert_prediction"]))
    
    print("Détail par méthode :")
    for method in df["method"].unique():
        sub = df[df["method"] == method]
        acc = accuracy_score(sub["ground_truth"], sub["sbert_prediction"])
        print(f" - {method.upper()} : {acc:.2%}")

    df.to_csv(results_file, index=False)
    print(f"Résultats détaillés sauvegardés : {results_file}")

# -------------------------------------------------------------------------
# Point d'entrée principal
# -------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="Script unifié d'évaluation de la qualité des reviews générées.")
    parser.add_argument("task", choices=["all", "hallucination", "structure", "spoiler", "sentiment", "sbert_prep", "sbert_eval"], 
                        help="La tâche à exécuter.")
    args = parser.parse_args()

    # Vérification du fichier d'entrée sauf pour sbert_eval (qui utilise les fichiers intermédiaires)
    if args.task != "sbert_eval" and not INPUT_FILE.exists():
        print(f"Fichier d'entrée manquant : {INPUT_FILE}")
        return

    # Chargement global des données
    df = None
    if args.task not in ["sbert_eval"]:
        print("Chargement des données...")
        df = pd.read_csv(INPUT_FILE)
        df = df[df["app_id"].isin(TARGET_APP_IDS)].copy()

    # Récupération des infos jeux
    game_details = {}
    if args.task in ["all", "hallucination", "structure", "spoiler", "sentiment"]:
        game_details = get_game_details(TARGET_APP_IDS)

    # Exécution des tâches
    if args.task in ["all", "hallucination"]:
        generate_hallucination_prompts(df, game_details)
    
    if args.task in ["all", "structure"]:
        generate_structure_prompts(df, game_details)
    
    if args.task in ["all", "spoiler"]:
        generate_spoiler_prompts(df, game_details)
    
    if args.task in ["all", "sentiment"]:
        generate_sentiment_prompts(df, game_details)
        
    if args.task in ["all", "sbert_prep"]:
        prepare_sbert_subset(df)
        
    if args.task in ["sbert_eval"]:
        if 'sentence_transformers' not in sys.modules:
            print("Erreur: les librairies sbert ne sont pas installées.")
        else:
            evaluate_sbert_accuracy()

if __name__ == "__main__":
    main()