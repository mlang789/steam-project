# ğŸ›¡ï¸ Guide d'Utilisation : Suite d'Ã‰valuation GenAI

Ce dossier contient trois scripts Python situÃ©s dans `src/evaluation/` permettant d'Ã©valuer la qualitÃ©, la diversitÃ© et le rÃ©alisme des reviews gÃ©nÃ©rÃ©es par vos modÃ¨les.

## ğŸ“‹ PrÃ©-requis Communs

Tous les scripts nÃ©cessitent un environnement Python avec les bibliothÃ¨ques suivantes :

```bash
pip install pandas numpy requests tqdm scikit-learn sentence-transformers tabulate
```

> **Note :** `sentence-transformers` est lourd mais nÃ©cessaire pour les analyses sÃ©mantiques (SBERT).

---

## 1. Ã‰valuation de la QualitÃ© & Jugement LLM

**Script :** `08_evaluate_quality.py`

Ce script gÃ©nÃ¨re des prompts pour qu'un "Juge IA" (GPT-4/Claude) dÃ©tecte les hallucinations, les spoilers ou les erreurs de structure. Il permet aussi de calculer une mÃ©trique de prÃ©cision de sentiment (SBERT vs Juge).

**Syntaxe :**
```bash
python src/evaluation/08_evaluate_quality.py [TACHE]
```

### TÃ¢ches Disponibles

| Argument | Description | Sorties (dans `evaluation/`) |
| :--- | :--- | :--- |
| `all` | ExÃ©cute toutes les gÃ©nÃ©rations de prompts + prÃ©paration SBERT. | Tous les fichiers ci-dessous. |
| `hallucination` | Prompts pour dÃ©tecter les faits inventÃ©s. | `prompts/batch_hallucination_*.txt` |
| `structure` | Prompts pour vÃ©rifier la rÃ¨gle "2 Positifs / 1 NÃ©gatif". | `prompts/batch_structure_*.txt` |
| `spoiler` | Prompts pour dÃ©tecter les spoilers narratifs. | `prompts/batch_spoiler_*.txt` |
| `sentiment` | Prompts pour vÃ©rifier l'alignement note/texte. | `prompts/batch_sentiment_naive.txt` |
| `sbert_prep` | PrÃ©pare l'Ã©chantillon pour l'Ã©valuation SBERT (voir ci-dessous). | `prompts/prompt_judge_sbert_300.txt`<br>`csv/sbert_subset_300_stratified.csv` |
| `sbert_eval` | Compare les prÃ©dictions SBERT vs Juge (nÃ©cessite Ã©tape manuelle). | `csv/sbert_evaluation_results_300.csv` |

### ğŸ§  Workflow SpÃ©cifique : Ã‰valuation SBERT
1.  Lancer `python src/evaluation/08_evaluate_quality.py sbert_prep`.
2.  Copier le contenu de `evaluation/prompts/prompt_judge_sbert_300.txt` dans ChatGPT/Claude.
3.  RÃ©cupÃ©rer **uniquement** le JSON de rÃ©ponse et le sauvegarder sous `evaluation/csv/judge_labels_300.json`.
4.  Lancer `python src/evaluation/08_evaluate_quality.py sbert_eval`.

---

## 2. Ã‰valuation de la DiversitÃ©

**Script :** `09_evaluate_diversity.py`

Mesure si le modÃ¨le "tourne en rond". Il analyse la richesse du vocabulaire (n-grams) et la redondance sÃ©mantique entre les reviews gÃ©nÃ©rÃ©es.

**Syntaxe :**
```bash
python src/evaluation/09_evaluate_diversity.py --input [FICHIER_CSV] --save [OPTIONS]
```

### Arguments ClÃ©s
*   `--inter-sim` : Active l'analyse sÃ©mantique SBERT (recommandÃ© pour dÃ©tecter si les reviews disent toutes la mÃªme chose).
*   `--save` : Ajoute les rÃ©sultats au rapport `results/results_diversity.md`.
*   `--prefix "Titre"` : Nom de l'expÃ©rience dans le rapport.

**Exemple complet :**
```bash
python src/evaluation/09_evaluate_diversity.py \
  --input reports/genai_inputs/prompt_batch_filled.csv \
  --inter-sim \
  --save \
  --prefix "Comparaison Naive vs Engineered"
```

---

## 3. Ã‰valuation du RÃ©alisme & Plagiat

**Script :** `10_evaluate_realism.py`

Compare vos reviews gÃ©nÃ©rÃ©es avec une base de "vraies" reviews Steam pour voir si elles "sonnent vrai" (proximitÃ© sÃ©mantique) et vÃ©rifier qu'elles ne sont pas du pur copier-coller (plagiat).

**Syntaxe :**
```bash
python src/evaluation/10_evaluate_realism.py --gen [CSV_IA] --real [CSV_REEL] --save [OPTIONS]
```

### Arguments ClÃ©s
*   `--gen` : Fichier des reviews gÃ©nÃ©rÃ©es.
*   `--real` : Fichier des reviews rÃ©elles (Ground Truth).
*   `--max-real 2000` : Limite le nombre de reviews rÃ©elles utilisÃ©es (conseillÃ© pour accÃ©lÃ©rer le calcul).
*   `--save` : Ajoute les rÃ©sultats au rapport `results/results_realism.md`.

**Exemple complet :**
```bash
python src/evaluation/10_evaluate_realism.py \
  --gen reports/genai_inputs/prompt_batch_filled.csv \
  --real data/raw/reviews_raw_train.csv \
  --max-real 2000 \
  --save \
  --prefix "Test de RÃ©alisme V1"
```

---

## ğŸ“‚ Architecture des Sorties

L'exÃ©cution de ces scripts peuple automatiquement l'arborescence suivante :

```text
PROJET_RACINE/
â”œâ”€â”€ evaluation/                 # Sorties du script Quality
â”‚   â”œâ”€â”€ csv/
â”‚   â”‚   â”œâ”€â”€ sbert_subset_300_stratified.csv
â”‚   â”‚   â”œâ”€â”€ judge_labels_300.json (Fichier MANUEL)
â”‚   â”‚   â””â”€â”€ sbert_evaluation_results_300.csv
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ batch_hallucination_*.txt
â”‚       â”œâ”€â”€ prompt_judge_sbert_300.txt
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ reports/                    # EntrÃ©es et Rapports Markdown
    â”œâ”€â”€ genai_inputs/           # Fichiers CSV d'entrÃ©e
    â”‚   â””â”€â”€ prompt_batch_filled.csv
    â””â”€â”€ results/                # Rapports cumulatifs (Diversity & Realism)
        â”œâ”€â”€ results_diversity.md
        â””â”€â”€ results_realism.md