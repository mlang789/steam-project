Tu es un expert, en ml avec python. Il faut faire un readme ultra complet pour ce projet (l'ancien readme n'est plus à jour). Analyse l'ensemble du projet stp.
Je veux : pas de smyleys, pas de superlatifs, que ce soit écrit comme si c'était un humain qui l'avait fait, et je veux pas de majuscules, UNIQUEMENT en début de phrase, pas de majuscules apres les double points par Exemple
Voici mon repo : 


================================================================================
FILE: requirements.txt
================================================================================

pandas
requests
tqdm
scikit-learn
matplotlib
seaborn
sentence-transformers
numpy
tabulate

================================================================================
FILE: README.md
================================================================================

# Steam Reviews – Sentiment Analysis & Conditional Generation

## Project overview
This project studies Steam game reviews from two complementary perspectives:

1) **Sentiment prediction (baseline)**  
We show that predicting whether a review is recommended or not from its text is a relatively easy task using classical machine learning methods.

2) **Conditional text generation (main contribution)**  
We explore how a language model can generate Steam-like reviews conditioned on a game title and a target rating (derived from Steam’s binary label), and how generation quality and controllability change with:
- naive prompting
- prompt engineering
- **fine-tuning (LoRA) on Steam reviews**

The project follows a progressive methodology: starting from a simple baseline, then moving toward more original generative experiments.

---

## Data
We use review-level data collected from the official Steam Reviews API.

Each review includes:
- the review text
- a binary recommendation label (recommended / not recommended)
- metadata such as playtime at review time and vote counts
- the Steam app ID

Game titles are retrieved from the Steam Store API and merged with the review data.

**Rating proxy (coarse):** Steam reviews are binary, so we map:
- `recommended = 1` → `rating = 9`
- `recommended = 0` → `rating = 3`

This creates a simple numeric conditioning signal for generation (positive vs negative) while staying realistic (non-extreme).

Raw and processed datasets are not versioned in this repository.  
They are generated locally using the scripts provided in `src/`.

---

## Repository structure
steam-project/
├── src/            Reproducible scripts (data collection, cleaning, baseline)
├── notebooks/      EDA and modeling notebooks
├── genai/          Prompting + fine-tuning utilities
├── reports/        Saved metrics and evaluation outputs (generated locally)
├── data/           Generated locally (ignored by git)
├── README.md
├── requirements.txt
└── .gitignore

---

## Baseline: sentiment prediction
A simple baseline model is implemented to predict the review recommendation label from text:
- TF-IDF text representation (unigrams and bigrams)
- Logistic regression classifier

Despite its simplicity, this baseline achieves a strong ROC-AUC score, confirming that sentiment information is largely explicit in review text. The baseline is used both as a reference point and as a diagnostic tool for evaluating generated reviews (sentiment compliance).

---

## Conditional review generation
The main contribution of this project is the study of conditional review generation.

Given a game title and a coarse numeric rating derived from the recommendation label, we generate synthetic Steam-like user reviews using a language model.

We compare multiple strategies:

### 1) Naive prompting
A single-sentence instruction:
- *"Write a Steam user review for the game X with rating Y/10."*

### 2) Prompt engineering
A structured prompt with explicit constraints (length, tone, no spoilers, etc.).  
Prompt experiments and qualitative examples are documented in `genai/prompts.md`.

### 3) Fine-tuning (LoRA, Option A)
We fine-tune a small open-source model using **LoRA in 4-bit** (Google Colab GPU).

**Important design choice (Option A):** fine-tuning focuses on **Steam-like style + target sentiment** (via rating 3 vs 9) without enforcing strict format constraints during training (real Steam reviews do not reliably follow constraints such as exact word count or exact “2 pros / 1 con” structure).  
Those constraints are instead applied at inference time via prompt engineering and evaluated separately.

The fine-tuning dataset is built from real reviews as prompt → completion pairs:
- prompt: “Write a positive/negative Steam review for game X (rating Y/10)”
- completion: the real Steam review text

Dataset builder:
- `genai/make_dataset.py` → outputs `reports/sft_train.jsonl` and `reports/sft_val.jsonl`

---

## Evaluation
Generated reviews are evaluated using:
- **automatic sentiment compliance**, measured with a lightweight classifier
- **simple structural checks** (e.g., word count constraints)
- qualitative analysis of realism, repetitiveness, and Steam-like style

### Automatic judges (TF-IDF vs SBERT)
We used two lightweight automatic sentiment judges:
- **TF-IDF + LogisticRegression** (bag-of-words baseline)
- **SBERT embeddings + LogisticRegression** (more robust to paraphrasing / style shifts)

**Balanced benchmark (v4, 25 positive + 25 negative prompts):**
- **Word-count compliance (100–140 words):** 0.72 (mean ~112 words)
- **Sentiment compliance (TF-IDF judge):** 0.54
- **Sentiment compliance (SBERT judge):** 0.60

This highlights both (i) the impact of decoding/format fixes on length control and (ii) the sensitivity of automatic evaluation: a bag-of-words judge can underestimate sentiment when generations use mixed phrasing, while embedding-based evaluation better captures overall meaning.

### Large-scale evaluation (v6_long, engineered prompts only)
We then ran a larger generation/evaluation to reduce variance and get stable estimates.

**Fine-tuned model (`finetuned_v6_long`), n = 4080 engineered prompts:**
- **Sentiment compliance (SBERT judge):** 0.866
- **Mean predicted probability (recommended):** 0.856
- **Mean word count:** 128.5
- **Word-count compliance (100–140 words):** 0.506

**Length diagnostics (v6_long rows):**
- Share `< 100` words: 0.113  
- Share `> 140` words: 0.382  

This confirms that fine-tuning strongly improves sentiment controllability at scale, while length control remains a decoding/prompting challenge (many generations overshoot 140 words).

Evaluation script:
- `genai/evaluate_generations.py`  
Outputs (prefix-based):
- `reports/generation_eval_*_rows.csv`
- `reports/generation_eval_*_summary.csv`

---

## How to run the project

### Install dependencies
pip install -r requirements.txt

### Collect and clean data
python src/collect_reviews.py
python src/clean_reviews.py
python src/enrich_games.py
python src/build_reviews_with_title.py

### Train the baseline model (TF-IDF judge)
python src/train_baseline_textclf.py

### Build a prompt batch for generation
python genai/build_prompt_batch.py

Fill `generated_text` in `reports/prompt_batch.csv` manually (or via an external generation workflow).

### Evaluate generated reviews (TF-IDF or SBERT judge)
python genai/merge_for_eval.py
python genai/evaluate_generations.py --input reports/prompt_batch_all.csv --prefix generation_eval_v4
python genai/evaluate_generations.py --input reports/prompt_batch_all.csv --prefix generation_eval_v4_sbert

### Build fine-tuning dataset (SFT)
python genai/make_dataset.py

Fine-tuning is performed in Google Colab (GPU) using LoRA/4-bit on a small open-source model (e.g., TinyLlama 1.1B Chat). The notebook/workflow can be reproduced from the commands and scripts in this repository.

---

## Notes
- The goal of generation is stylistic and sentiment coherence rather than factual accuracy.
- Evaluation is intentionally lightweight and exploratory (automatic checks + qualitative inspection).
- Fine-tuning uses a pragmatic setup (LoRA + 4-bit quantization) suitable for student GPU environments (Colab).
- Further improvements would focus on tighter length control (e.g., decoding constraints such as `min_new_tokens` / lower `max_new_tokens`, or post-processing for strict word-count compliance).


================================================================================
FILE: analyse.md
================================================================================

# Analyse des Performances des Modèles GenAI

Ce document présente une évaluation approfondie des trois approches de génération de reviews Steam (Naïve, Engineered, Fine-tuned). L'analyse porte sur la qualité factuelle, le respect des structures logiques, la cohérence sémantique, ainsi que le réalisme et la diversité des textes produits.

## 1. Analyse des Hallucinations et de la Qualité Factuelle

Cette section mesure la tendance des modèles à inventer des faits inexistants (mécaniques de jeu, modes, personnages).

### Comparaison des taux d'hallucination

| Modèle | Jeu | Taux d'Hallucination | Moyenne Globale | Principales Typologies d'Erreurs |
| :--- | :--- | :---: | :---: | :--- |
| **Naïf** (Zéro-Shot) | TF2 <br> CS2 | 56% <br> 60% | **58%** | **Confusion de genre :** invention de modes "histoire", mécaniques RPG (niveaux, arbres de compétences) et personnages de jeux tiers. |
| **Engineered** (Instructions) | TF2 <br> CS2 | 68% <br> 62% | **65%** | **Détails fictifs accrus :** invention de classes, de développeurs et de mécaniques complexes (grappins, selfies). |
| **Fine-tuned** (Entraîné) | TF2 <br> CS2 | 28% <br> 34% | **31%** | **Résidus techniques :** fuites de métadonnées, confusions de plateformes ou chiffres erronés, mais respect global du genre. |

**Observations majeures :**

1.  **Le paradoxe de l'ingénierie (+7%) :** Le modèle "Engineered" présente le taux d'erreur le plus élevé (65%). En forçant le modèle à être spécifique via le prompt, on le contraint à générer des détails qu'il ne maîtrise pas, provoquant une sur-créativité factuellement fausse.
2.  **L'efficacité du fine-tuning (-34%) :** L'entraînement sur de réelles données réduit le taux d'hallucination de plus de moitié. Le modèle intègre le lexique et les limites du jeu, éliminant les inventions hors-sujet comme les mondes ouverts dans des jeux de tir.
3.  **Thématiques récurrentes :** Les modèles non entraînés tendent à transformer tout jeu en RPG narratif. Le modèle fine-tuned corrige ce biais mais souffre parfois de pollution de données (apparitions d'identifiants ou d'artefacts techniques).

---

## 2. Respect des Instructions Logiques et Structurelles

Cette partie évalue la capacité des modèles à suivre des contraintes strictes : structure argumentative (2 points positifs, 1 négatif) et interdiction de spoilers.

### Comparaison de l'adhérence au prompt

| Modèle | Jeu | Échec de Structure (2+ / 1-) | Taux de "Faux Spoilers" | Observation Clé |
| :--- | :--- | :---: | :---: | :--- |
| **Naïf** | Dota 2 <br> CS2 | N/A (Prompt libre) | 14% <br> 22% | Invente des scénarios par réflexe narratif. |
| **Engineered** | Dota 2 <br> CS2 | **100%** <br> **96%** | 10% <br> 36% | **Échec quasi-total** sur la logique quantitative. |
| **Fine-tuned** | Dota 2 <br> CS2 | **90%** <br> **82%** | 4% <br> 10% | **Amélioration nette** de la compréhension structurelle. |

**Analyse :**

*   **Limites du prompt seul :** L'instruction explicite "exactement 2 points positifs et 1 négatif" échoue dans 98% des cas avec le modèle Engineered. Les modèles de taille réduite (1.1B paramètres) privilégient la fluidité linguistique à la rigueur arithmétique.
*   **Apport du fine-tuning :** Le taux de réussite sur la structure passe de ~2% à ~14%. Bien que le score reste modeste, le modèle tente d'équilibrer ses arguments.
*   **Gestion des spoilers :** L'instruction "pas de spoilers" a paradoxalement incité les modèles Naïf et Engineered à inventer une histoire pour pouvoir en parler. Le fine-tuning corrige ce défaut : ayant appris que des jeux comme CS2 n'ont pas de campagne narrative, le taux d'hallucination narrative chute drastiquement (de 36% à 4% sur CS2).

---

## 3. Cohérence Sémantique (Note vs Texte)

Nous analysons ici l'alignement entre la note attribuée (input) et le sentiment dégagé par le texte généré.

### Taux de conformité entre note cible et sentiment

| Jeu Évalué | Note Cible | Sentiment Attendu | Taux de Succès | Nature des Divergences |
| :--- | :---: | :---: | :---: | :--- |
| **Dota 2** | 9 / 10 | POSITIF | **100%** | Aucune. |
| **Dota 2** | 3 / 10 | NÉGATIF | **80%** | Incohérence interne (ex: "étonné par la qualité" pour un 3/10). |
| **Team Fortress 2** | 9 / 10 | POSITIF | **100%** | Aucune. |
| **Team Fortress 2** | 3 / 10 | NÉGATIF | **84%** | Conflit affectif (ex: "mon jeu préféré" pour un 3/10). |
| **Counter-Strike 2** | 3 / 10 | NÉGATIF | **76%** | Hallucinations de satisfaction. |
| **MOYENNE** | - | - | **88.0%** | **Biais de positivité prédominant.** |

**Interprétation :**
Un biais de complaisance est observé. Le modèle excelle dans la génération positive (100% de succès) mais montre une résistance à la critique. Environ 20% des reviews négatives contiennent des éloges contradictoires. Cela suggère que l'alignement initial du modèle de base (TinyLlama) favorise un ton utile et enthousiaste, créant une dissonance cognitive lors de la génération de critiques sévères.

---

## 4. Analyse du Réalisme et de la Diversité

Cette section compare la proximité des textes avec de vraies reviews humaines et leur variété lexicale.

### Synthèse du réalisme (Sémantique)
*   **Naïf & Engineered (Score ~0.69) :** Ces approches produisent des textes originaux qui ne copient pas le corpus existant. Aucun cas de plagiat n'est détecté.
*   **Fine-tuned (Score ~0.70) :** Le fine-tuning offre le réalisme sémantique le plus élevé. Une infime trace de mémorisation (0.02% de chevauchement) est détectée, ce qui est négligeable mais indique que le modèle commence à retenir des phrases types du dataset d'entraînement.

### Synthèse de la diversité
*   **Diversité Lexicale (Interne) :** Les modèles Naïf et Engineered utilisent un vocabulaire plus varié localement (plus de mots uniques par review). Le fine-tuning réduit légèrement cette variété lexicale.
*   **Diversité Sémantique (Globale) :**
    *   Les modèles à prompts (Naïf/Engineered) ont une similarité inter-review élevée (~0.84). Ils tendent à répéter les mêmes structures et arguments pour tous les jeux.
    *   Le modèle **Fine-tuned** présente une similarité plus faible (~0.77). Cela signifie qu'il génère des reviews plus distinctes les unes des autres.

**Conclusion sur la diversité :** Le fine-tuning produit des textes individuellement plus simples, mais globalement plus variés et moins répétitifs que les approches basées sur le prompt engineering.

---

## 5. Fiabilité de l'Évaluation Automatisée (Juge SBERT)

Pour valider nos métriques, nous avons comparé les prédictions du classifieur interne (SBERT) avec celles d'un juge expert (LLM externe) sur un échantillon de 300 reviews.

| Segment de Données | Accuracy | F1-Score (Positif) | F1-Score (Négatif) | Fiabilité |
| :--- | :---: | :---: | :---: | :--- |
| **Global** | **87.67%** | **0.92** | **0.69** | **Élevée** |
| **Sous-groupe : Naïf** | 93.00% | 0.96 | 0.81 | Très Élevée |
| **Sous-groupe : Engineered** | 84.00% | 0.89 | 0.62 | Modérée |
| **Sous-groupe : Fine-tuned** | 86.00% | 0.91 | 0.65 | Élevée |

**Validité de l'outil :**
Le juge SBERT est robuste pour l'évaluation globale. On note cependant un déséquilibre : il détecte mieux les avis positifs (F1: 0.92) que les avis négatifs (F1: 0.69), souvent plus nuancés ou sarcastiques. La précision baisse légèrement sur les textes complexes (Engineered/Fine-tuned) par rapport aux textes stéréotypés du modèle Naïf.

================================================================================
FILE: src/USAGE.md
================================================================================

# Documentation Complète du Projet : Steam Reviews GenAI

Ce projet est un pipeline complet de Machine Learning et d'IA Générative. Il est structuré en 4 grandes phases :

1.  **Data Engineering** : Collecte et préparation des données Steam.
2.  **Juge IA** : Entraînement de classifieurs (SBERT & TF-IDF) pour évaluer la qualité.
3.  **GenAI** : Fine-tuning d'un LLM et génération de reviews.
4.  **Évaluation** : Analyse de la qualité, de la diversité et du réalisme des textes générés.

---

## Structure du Projet

Voici l'organisation globale des fichiers :

```text
steam-project/
├── data/                      # Stockage des données
│   ├── raw/                   # JSON/CSV bruts de l'API Steam
│   ├── processed/             # Données nettoyées (dataset_train.csv, titles.csv)
│   └── outputs_gen_ai_models/ # Réception des fichiers venant de Colab
├── evaluation/                # Sorties spécifiques aux scripts d'évaluation
│   ├── csv/                   # Résultats intermédiaires (SBERT, JSON manuel)
│   └── prompts/               # Fichiers textes pour évaluation par LLM externe
├── reports/                   # Artefacts et Résultats
│   ├── models/                # Modèles du Juge (.joblib)
│   ├── genai_inputs/          # Fichiers d'entrée pour le LLM (prompts, jsonl)
│   └── results/               # Rapports de performance (Markdown)
├── src/                       # Code Source
│   ├── config.py              # Configuration globale
│   ├── utils.py               # Utilitaires (SBERT, etc.)
│   ├── data/                  # Pipeline ETL (Extract-Transform-Load)
│   ├── judge/                 # Classification (Le Juge)
│   ├── genai/                 # Pipeline GenAI (Entraînement & Inférence)
│   └── evaluation/            # Scripts d'analyse (Qualité, Diversité, Réalisme)
```

---

## Phase 1 : Pipeline de Données

Cette phase transforme les données brutes d'Internet en un dataset propre pour le Machine Learning.

### Étape 1 : Collecte (`01_collect.py`)

Télécharge les reviews et les titres des jeux depuis l'API Steam.

*   **Commandes :**
    ```bash
    python src/data/01_collect.py train
    python src/data/01_collect.py validation
    ```
*   **Outputs :**
    *   `data/raw/reviews_raw_train.csv`
    *   `data/processed/titles_train.csv` (Titres des jeux)

### Étape 2 : Construction du Dataset (`02_process.py`)

Nettoie les textes, fusionne avec les titres, et convertit le vote (recommandé/non) en note (3/10 ou 9/10).

*   **Commandes :**
    ```bash
    python src/data/02_process.py train
    python src/data/02_process.py validation
    ```
*   **Outputs :**
    *   `data/processed/dataset_train.csv` : **Le fichier maître**.
    *   `data/processed/dataset_val.csv` : Pour tests finaux.

---

## Phase 2 : Entraînement du Juge

Création d'une IA capable de dire si une review est positive ou négative. Nous entraînons deux versions pour comparer l'évaluation.

### Étape 3 : Entraînement (`03_train.py`)

#### Option A : Modèle SBERT (Recommandé)
Capture la sémantique (le sens) des phrases.

*   **Commande :**
    ```bash
    python src/judge/03_train.py --model sbert
    ```
*   **Output :** `reports/models/judge_model_sbert.joblib`

#### Option B : Modèle TF-IDF (Baseline)
Se base sur les mots-clés. Utile pour vérifier si le LLM "triche" en utilisant juste des mots positifs.

*   **Commande :**
    ```bash
    python src/judge/03_train.py --model tfidf
    ```
*   **Output :** `reports/models/judge_model_tfidf.joblib`

---

## Phase 3 : Generative AI (Local & Colab)

Cette phase est hybride : préparation en local, entraînement/génération sur Google Colab (GPU), puis retour en local pour l'évaluation.

### Étape 4 : Préparation des Données (Local)

#### A. Création des Prompts "Zero-Shot" (`04_generate_prompts.py`)
Génère les instructions que le LLM devra suivre (Naive vs Engineered).

*   **Commande :**
    ```bash
    python src/genai/04_generate_prompts.py
    ```
*   **Output :** `reports/genai_inputs/prompt_batch.csv`

#### B. Préparation du Dataset de Fine-Tuning (`05_prepare_training.py`)
Transforme les vraies reviews en format JSONL pour ré-entraîner le LLM (Instruction Tuning).

*   **Commande :**
    ```bash
    python src/genai/05_prepare_training.py
    ```
*   **Output :** `reports/genai_inputs/sft_train.jsonl`

### Étape 5 : Exécution sur Google Colab (GPU requis)

Utilisez les fichiers générés à l'étape 4.

#### Notebook 1 : Entraînement (Fine-Tuning LoRA)
1.  **Upload :** Déposez `reports/genai_inputs/sft_train.jsonl` sur le Colab.
2.  **Entraînement :** Lancez le notebook pour fine-tuner `TinyLlama-1.1B`.
3.  **Sauvegarde :** Récupérez le dossier de l'adaptateur LoRA sur votre Google Drive.

#### Notebook 2 : Inférence & Génération
1.  **Upload :** Déposez `reports/genai_inputs/prompt_batch.csv` sur le Colab.
2.  **Génération :** Lancez le notebook pour générer les 3 fichiers CSV :
    *   `generated_reviews_naive.csv`
    *   `generated_reviews_engineered.csv`
    *   `generated_reviews_finetuned.csv`
3.  **Download :** Téléchargez ces 3 fichiers sur votre ordinateur local.

### Étape 6 : Consolidation (Local)

#### C. Fusion des Résultats (`06_merge_outputs.py`)
1.  **Action :** Placez les 3 fichiers CSV du Colab dans `data/outputs_genai_models/`.
2.  **Commande :**
    ```bash
    python src/genai/06_merge_outputs.py
    ```
3.  **Output :** `reports/genai_inputs/prompt_batch_filled.csv`

---

## Phase 4 : Suite d'Évaluation

Cette phase utilise les scripts situés dans `src/evaluation/` pour analyser la qualité, la diversité et le réalisme des reviews générées.

**Pré-requis** :
```bash
pip install pandas numpy requests tqdm scikit-learn sentence-transformers tabulate
```

### Étape 7 : Évaluation de la Qualité & Jugement LLM (`08_evaluate_quality.py`)

Ce script génère des prompts pour qu'un "Juge IA" externe (ex: GPT-4) détecte les hallucinations ou erreurs, et prépare la comparaison avec le Juge SBERT interne.

**Syntaxe :**
```bash
python src/evaluation/08_evaluate_quality.py [TACHE]
```

**Tâches Disponibles :**

| Argument | Description | Sorties (dans `evaluation/`) |
| :--- | :--- | :--- |
| `all` | Exécute toutes les générations de prompts + préparation SBERT. | Tous les fichiers ci-dessous. |
| `hallucination` | Prompts pour détecter les faits inventés. | `prompts/batch_hallucination_*.txt` |
| `structure` | Prompts pour vérifier la règle structurelle. | `prompts/batch_structure_*.txt` |
| `spoiler` | Prompts pour détecter les spoilers narratifs. | `prompts/batch_spoiler_*.txt` |
| `sentiment` | Prompts pour vérifier l'alignement note/texte. | `prompts/batch_sentiment_naive.txt` |
| `sbert_prep` | Prépare l'échantillon pour l'évaluation SBERT. | `prompts/prompt_judge_sbert_300.txt`<br>`csv/sbert_subset_300_stratified.csv` |
| `sbert_eval` | Compare les prédictions SBERT vs Juge externe. | `csv/sbert_evaluation_results_300.csv` |

**Workflow Spécifique : Évaluation SBERT**
1.  Lancer `python src/evaluation/08_evaluate_quality.py sbert_prep`.
2.  Copier le contenu de `evaluation/prompts/prompt_judge_sbert_300.txt` dans ChatGPT/Claude.
3.  Récupérer **uniquement** le JSON de réponse et le sauvegarder sous `evaluation/csv/judge_labels_300.json`.
4.  Lancer `python src/evaluation/08_evaluate_quality.py sbert_eval`.

### Étape 8 : Évaluation de la Diversité (`09_evaluate_diversity.py`)

Mesure la richesse du vocabulaire (n-grams) et la redondance sémantique entre les reviews générées pour vérifier si le modèle se répète.

**Exemple complet :**
```bash
python src/evaluation/09_evaluate_diversity.py \
  --input reports/genai_inputs/prompt_batch_filled.csv \
  --inter-sim \
  --save \
  --prefix "Comparaison Naive vs Engineered"
```

**Arguments Clés :**
*   `--inter-sim` : Active l'analyse sémantique SBERT (recommandé).
*   `--save` : Ajoute les résultats au rapport `reports/results/results_diversity.md`.
*   `--prefix` : Nom de l'expérience dans le rapport.

### Étape 9 : Évaluation du Réalisme & Plagiat (`10_evaluate_realism.py`)

Compare les reviews générées avec une base de "vraies" reviews Steam (Ground Truth) pour mesurer la proximité sémantique et vérifier l'absence de plagiat pur.

**Exemple complet :**
```bash
python src/evaluation/10_evaluate_realism.py \
  --gen reports/genai_inputs/prompt_batch_filled.csv \
  --real data/raw/reviews_raw_train.csv \
  --max-real 2000 \
  --save \
  --prefix "Test de Réalisme V1"
```

**Arguments Clés :**
*   `--gen` : Fichier des reviews générées.
*   `--real` : Fichier des reviews réelles.
*   `--max-real` : Limite le nombre de reviews réelles utilisées (conseillé : 2000).
*   `--save` : Ajoute les résultats au rapport `reports/results/results_realism.md`.

---

## Résumé du Flux de Données

```mermaid
graph TD
    A[API Steam] -->|src/data_pipeline| B(data/processed/dataset_train.csv)
    
    B -->|src/judge| C[reports/models/judge_model_sbert.joblib]
    
    B -->|src/genai| E[reports/genai_inputs/sft_train.jsonl]
    B -->|src/genai| F[reports/genai_inputs/prompt_batch.csv]
    
    E & F -.->|Google Colab| G[data/outputs_genai_models/*.csv]
    
    G -->|src/genai/merge| H(prompt_batch_filled.csv)
    
    H & C -->|src/evaluation/quality| I[Rapport Qualité SBERT]
    H -->|src/evaluation/diversity| J[Rapport Diversité]
    H & B -->|src/evaluation/realism| K[Rapport Réalisme]
```

================================================================================
FILE: src/config.py
================================================================================

# FILE: src/config.py

from pathlib import Path

# Configuration des Jeux
TRAIN_APP_IDS = [570, 730, 440, 1091500, 1245620, 1517290, 1599340, 2357570, 1272080, 1547000]
VALIDATION_APP_IDS = [548430, 892970, 1097840, 1940340]

# Chemins de base
# Astuce : On remonte d'un cran car config.py est maintenant dans src/
PROJECT_ROOT = Path(__file__).resolve().parent.parent 
DATA_DIR = PROJECT_ROOT / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
EXTERNAL_OUTPUTS_DIR = DATA_DIR / "outputs_gen_ai_models" # On garde ça

# Organisation de Reports (Outputs)
REPORTS_DIR = PROJECT_ROOT / "reports"
MODELS_DIR = REPORTS_DIR / "models"           # Pour les .joblib
GENAI_INPUTS_DIR = REPORTS_DIR / "genai_inputs" # Pour les JSONL et Prompts
EVAL_DIR = REPORTS_DIR / "results"         # Pour les résultats

# Création automatique des dossiers
for d in [RAW_DIR, PROCESSED_DIR, EXTERNAL_OUTPUTS_DIR, MODELS_DIR, GENAI_INPUTS_DIR, EVAL_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# Dictionnaire de fichiers
FILES = {
    "train": {
        "raw_reviews": RAW_DIR / "reviews_raw_train.csv",
        "titles": PROCESSED_DIR / "titles_train.csv",   
        "final": PROCESSED_DIR / "dataset_train.csv"
    },
    "validation": {
        "raw_reviews": RAW_DIR / "reviews_raw_val.csv",
        "titles": PROCESSED_DIR / "titles_val.csv",      
        "final": PROCESSED_DIR / "dataset_val.csv"
    }
}

================================================================================
FILE: src/utils.py
================================================================================

import numpy as np
from sentence_transformers import SentenceTransformer

class SBERTEmbedder:
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        emb = self.model.encode(
            list(X),
            normalize_embeddings=True,
            show_progress_bar=True,
        )
        return np.asarray(emb)


================================================================================
FILE: src/judge/03_train.py
================================================================================

import sys
from pathlib import Path
# Ajout du chemin racine pour trouver src.config
sys.path.append(str(Path(__file__).resolve().parents[2]))

import argparse
import pandas as pd
import joblib
import numpy as np
from pathlib import Path
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, roc_auc_score
from sentence_transformers import SentenceTransformer
from src.config import FILES, MODELS_DIR, EVAL_DIR
from src.utils import SBERTEmbedder

def main():
    # On fixe la seed pour la reproductibilité
    np.random.seed(42)

    parser = argparse.ArgumentParser(description="Entraîner un modèle Juge (SBERT ou TF-IDF)")
    parser.add_argument("--model", type=str, choices=["sbert", "tfidf"], default="sbert")
    args = parser.parse_args()

    # --- Chargement du dataset train ---
    df_train = pd.read_csv(FILES["train"]["final"])
    # Gestion des valeurs manquantes et conversion en string
    df_train["review_text"] = df_train["review_text"].fillna("").astype(str)
    X_train_full = df_train["review_text"]
    # Création de la cible binaire (positif si note > 5)
    y_train_full = (df_train["rating"] > 5).astype(int)

    # --- Définition du modèle ---
    base_clf = LinearSVC(class_weight="balanced", max_iter=5000, random_state=42)

    # Construction du pipeline selon l'argument passé
    if args.model == "sbert":
        steps = [
            ("embed", SBERTEmbedder()),
            ("clf", base_clf)
        ]
    else:
        steps = [
            ("tfidf", TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)),
            ("clf", base_clf)
        ]

    pipe = Pipeline(steps)

    # --- Entraînement sur tout le train ---
    print("Entraînement sur tout le dataset TRAIN...")
    pipe.fit(X_train_full, y_train_full)

    # --- Chargement du dataset validation finale ---
    df_val = pd.read_csv(FILES["validation"]["final"])
    df_val["review_text"] = df_val["review_text"].fillna("").astype(str)
    X_val = df_val["review_text"]
    y_val = (df_val["rating"] > 5).astype(int)

    # --- Evaluation ---
    # on récupère les scores de décision pour calculer l'auc
    if args.model == "sbert":
        scores = pipe.named_steps['clf'].decision_function(pipe.named_steps['embed'].transform(X_val))
    else:
        scores = pipe.named_steps['clf'].decision_function(pipe.named_steps['tfidf'].transform(X_val))

    y_pred_val = (scores >= 0).astype(int)
    auc = roc_auc_score(y_val, scores)
    report = classification_report(y_val, y_pred_val, digits=4)

    print(f"\n--- Évaluation sur VALIDATION finale ---")
    print(report)
    print(f"ROC-AUC Score : {auc:.4f}")

    # --- Sauvegarde des métriques ---
    metrics_path = EVAL_DIR / f"metrics_{args.model}.txt"

    with open(metrics_path, "w", encoding="utf-8") as f:
        f.write(f"Model: {args.model}\n")
        f.write(f"Threshold: 0 (implicit)\n")
        f.write(f"ROC-AUC: {auc:.6f}\n\n")
        f.write("Classification report:\n")
        f.write(report)

    print(f"Métriques sauvegardées dans : {metrics_path}")

    # --- Sauvegarde du modèle ---
    out_path = MODELS_DIR / f"judge_model_{args.model}.joblib"
    joblib.dump(pipe, out_path)
    print(f"Modèle sauvegardé sous : {out_path}")

if __name__ == "__main__":
    main()

================================================================================
FILE: src/genai/04_generate_prompts.py
================================================================================

import sys
from pathlib import Path

# ajout du chemin racine pour trouver src.config
sys.path.append(str(Path(__file__).resolve().parents[2]))

import pandas as pd
from src.config import FILES, GENAI_INPUTS_DIR

DATA_PATH = FILES["train"]["final"]
OUT_PATH = GENAI_INPUTS_DIR / "prompt_batch.csv"

# nombre de prompts à générer par jeu et par type de note
N_PER_GAME_PER_RATING = 1000

# notes utilisées dans le dataset
POS_RATING = 9
NEG_RATING = 3

# descriptions manuelles associées aux identifiants steam
GAME_DESCRIPTIONS_BY_APP_ID = {
    570: (
        "Dota 2 is a free-to-play 5v5 MOBA where two teams fight to destroy the enemy Ancient. "
        "Matches revolve around laning, farming, objectives, teamfights, and a huge roster of heroes with deep mechanics."
    ),
    730: (
        "Counter-Strike 2 is a competitive 5v5 tactical FPS focused on precise gunplay, teamwork, and economy management. "
        "Rounds are usually bomb defusal, where small decisions and coordination can decide the game."
    ),
    440: (
        "Team Fortress 2 is a class-based multiplayer shooter with 9 distinct classes and objective-driven modes like Payload and Control Points. "
        "It has a stylized tone, chaotic fights, and a strong focus on class synergy and funny emergent moments."
    ),
    1091500: (
        "Cyberpunk 2077 is an open-world action RPG set in Night City. "
        "You play in first-person, mix narrative quests with combat and stealth, and build your character through gear, skills, and cyberware."
    ),
    1245620: (
        "Elden Ring is an open-world action RPG by FromSoftware with challenging combat, large-scale exploration, and tough bosses. "
        "It emphasizes learning enemy patterns, building your character, and discovering secrets across a dark fantasy world."
    ),
}

# correspondance de secours via le titre si l'identifiant est manquant
GAME_DESCRIPTIONS_BY_TITLE_KEYWORD = {
    "dota 2": GAME_DESCRIPTIONS_BY_APP_ID[570],
    "counter-strike 2": GAME_DESCRIPTIONS_BY_APP_ID[730],
    "cs2": GAME_DESCRIPTIONS_BY_APP_ID[730],
    "team fortress 2": GAME_DESCRIPTIONS_BY_APP_ID[440],
    "tf2": GAME_DESCRIPTIONS_BY_APP_ID[440],
    "cyberpunk 2077": GAME_DESCRIPTIONS_BY_APP_ID[1091500],
    "elden ring": GAME_DESCRIPTIONS_BY_APP_ID[1245620],
}

def get_game_description(app_id: int, title: str) -> str:
    # 1. priorité à la correspondance par identifiant
    if app_id in GAME_DESCRIPTIONS_BY_APP_ID:
        return GAME_DESCRIPTIONS_BY_APP_ID[app_id]

    # 2. recherche par mot-clé dans le titre
    title_l = (title or "").strip().lower()
    for k, desc in GAME_DESCRIPTIONS_BY_TITLE_KEYWORD.items():
        if k in title_l:
            return desc

    # 3. description générique par défaut
    return (
        "This is a video game on Steam. Write the review based on what a regular player would realistically experience."
    )

def make_naive_prompt(title: str, rating: int) -> str:
    return f'Write a Steam user review for the game "{title}" with rating {rating}/10.'

def make_engineered_prompt(title: str, rating: int, game_description: str) -> str:
    # structure : 2 points positifs et 1 négatif (ou l'inverse selon la note)
    if rating >= 7:
        point_guideline = (
            "Naturally include 2 things you genuinely liked and 1 thing you disliked (worked into the text, not as a list)."
        )
    else:
        point_guideline = (
            "Naturally include 2 things you genuinely disliked and 1 thing you liked (worked into the text, not as a list)."
        )

    return "\n".join(
        [
            "You are a real Steam user writing a review after actually playing the game.",
            "Write as human as possible: include personal experience, small details, and a natural voice.",
            "",
            f'Game: "{title}"',
            f"Target rating: {rating}/10",
            "",
            "Game description (use this to ground your review):",
            game_description,
            "",
            "Guidelines:",
            point_guideline,
            "- Mention at least one concrete personal detail (e.g., playtime estimate, playing solo vs with friends, a memorable moment, performance/controls feel, difficulty spike).",
            "- Sound casual and honest (not marketing). It's okay to be a bit imperfect or nuanced.",
            "- No spoilers (don’t reveal major story beats or endings).",
            "- Output only the review text (no title, no bullets).",
        ]
    )

def main() -> None:
    if not DATA_PATH.exists():
        raise FileNotFoundError(f"Missing file: {DATA_PATH}")

    df = pd.read_csv(DATA_PATH)

    required_cols = {"app_id", "title", "rating"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"Missing columns in dataset: {sorted(missing)}")

    # filtrage pour ne garder que les notes cibles
    df = df[df["rating"].isin([NEG_RATING, POS_RATING])].copy()

    rows = []

    # échantillonnage par groupe de jeu et de note
    for (app_id, title, rating), group in df.groupby(
        ["app_id", "title", "rating"], dropna=False
    ):
        sample_n = min(N_PER_GAME_PER_RATING, len(group))
        if sample_n == 0:
            continue

        sampled = group.sample(n=sample_n, random_state=42)

        # récupération de la description unique pour ce groupe
        safe_app_id = int(app_id) if pd.notna(app_id) else -1
        safe_title = str(title)
        game_desc = get_game_description(safe_app_id, safe_title)

        for _, _row in sampled.iterrows():
            # génération du prompt naïf
            rows.append(
                {
                    "app_id": safe_app_id,
                    "title": safe_title,
                    "rating": int(rating),
                    "method": "naive",
                    "prompt": make_naive_prompt(safe_title, int(rating)),
                    "generated_text": "",
                }
            )

            # génération du prompt travaillé
            rows.append(
                {
                    "app_id": safe_app_id,
                    "title": safe_title,
                    "rating": int(rating),
                    "method": "engineered",
                    "prompt": make_engineered_prompt(safe_title, int(rating), game_desc),
                    "generated_text": "",
                }
            )

    out_df = pd.DataFrame(rows)

    out_df.to_csv(OUT_PATH, index=False, encoding="utf-8")
    print(f"Saved {OUT_PATH} with shape: {out_df.shape}")
    print(out_df.head(6).to_string(index=False))

if __name__ == "__main__":
    main()

================================================================================
FILE: src/genai/05_prepare_training.py
================================================================================

import sys
from pathlib import Path
# ajout du chemin racine pour trouver src.config
sys.path.append(str(Path(__file__).resolve().parents[2]))

import json
import pandas as pd
from sklearn.model_selection import train_test_split
from src.config import FILES, GENAI_INPUTS_DIR

# on utilise le dataset final généré par l'étape 02
DATA_PATH = FILES["train"]["final"]

# on sauvegarde dans le dossier propre
TRAIN_PATH = GENAI_INPUTS_DIR / "sft_train.jsonl"
VAL_PATH = GENAI_INPUTS_DIR / "sft_val.jsonl"

# définitions des constantes (notes et limites de caractères)
POS_RATING = 9
NEG_RATING = 3
ALLOWED_RATINGS = {POS_RATING, NEG_RATING}

MIN_REVIEW_CHARS = 80
MAX_REVIEW_CHARS = 1200

def make_sft_prompt(title: str, rating: int) -> str:
    # construction du prompt instruction pour le fine-tuning
    sentiment = "positive" if rating >= 7 else "negative"
    return "\n".join([
        "You are a regular Steam user.",
        "",
        f'Write a {sentiment} Steam user review for the game "{title}".',
        f"Target rating: {rating}/10.",
        "",
        "Guidelines:",
        "- Casual, honest tone",
        "- No spoilers",
        "- Output only the review text",
        "",
        "Review:",
    ])

def write_jsonl(path: Path, rows: list[dict]) -> None:
    # écriture du fichier au format jsonl
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def main() -> None:
    # vérification de l'existence du fichier source
    if not DATA_PATH.exists():
        raise FileNotFoundError(f"Missing file: {DATA_PATH}")

    # chargement et validation des colonnes requises
    df = pd.read_csv(DATA_PATH)
    required = {"title", "rating", "review_text"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Missing columns: {sorted(missing)}")

    # conversion des types et gestion des valeurs nulles
    df["review_text"] = df["review_text"].fillna("").astype(str)
    df["title"] = df["title"].fillna("").astype(str)
    df["rating"] = df["rating"].astype(int)

    # filtrage : on garde seulement les notes cibles et les textes de bonne longueur
    df = df[df["rating"].isin(ALLOWED_RATINGS)].copy()
    df = df[df["review_text"].str.len() >= MIN_REVIEW_CHARS].copy()
    df = df[df["review_text"].str.len() <= MAX_REVIEW_CHARS].copy()
    df = df[df["title"].str.strip().str.len() > 0].copy()

    if df.empty:
        raise ValueError("No rows left after filtering.")

    # construction des paires prompt / completion
    rows = []
    for _, r in df.iterrows():
        prompt = make_sft_prompt(r["title"], int(r["rating"]))
        completion = r["review_text"].strip()
        rows.append({"prompt": prompt, "completion": completion})

    # division du jeu de données (90% train, 10% validation)
    train_rows, val_rows = train_test_split(rows, test_size=0.1, random_state=42)

    # sauvegarde sur le disque
    write_jsonl(TRAIN_PATH, train_rows)
    write_jsonl(VAL_PATH, val_rows)

    print(f"Saved:\n- {TRAIN_PATH} ({len(train_rows)} rows)\n- {VAL_PATH} ({len(val_rows)} rows)")

if __name__ == "__main__":
    main()

================================================================================
FILE: src/genai/06_merge_outputs.py
================================================================================

import sys
from pathlib import Path
# ajout du chemin racine pour l'import des modules
sys.path.append(str(Path(__file__).resolve().parents[2]))

import pandas as pd
from src.config import EXTERNAL_OUTPUTS_DIR, GENAI_INPUTS_DIR

# définition des chemins d'entrée et de sortie
INPUT_DIR = EXTERNAL_OUTPUTS_DIR
OUT_PATH = GENAI_INPUTS_DIR / "prompt_batch_filled.csv"

# mapping des méthodes vers les noms de fichiers générés
FILES_MAP = {
    "naive": "generated_reviews_naive.csv",
    "engineered": "generated_reviews_engineered.csv",
    "finetuned": "generated_reviews_finetuned.csv"
}

def main():
    print(f"--- Fusion des fichiers depuis : {INPUT_DIR} ---")
    
    # vérification du dossier source
    if not INPUT_DIR.exists():
        raise FileNotFoundError(f"Le dossier {INPUT_DIR} est introuvable.")

    dfs = []
    # parcours des fichiers définis dans le mapping
    for method, filename in FILES_MAP.items():
        file_path = INPUT_DIR / filename
        if file_path.exists():
            print(f"Chargement de '{method}' : {filename}")
            try:
                # lecture du fichier et ajout de la colonne identifiant la méthode
                df = pd.read_csv(file_path, encoding="utf-8", engine="python")
                df["method"] = method
                
                # on ne garde que les fichiers valides contenant du texte
                if "generated_text" in df.columns:
                    dfs.append(df)
                else:
                    print(f"Colonne manquante dans {filename}")
            except Exception as e:
                print(f"Erreur : {e}")
        else:
            print(f"Manquant : {filename}")

    if dfs:
        # fusion des différents datasets en un seul
        full_df = pd.concat(dfs, ignore_index=True)
        full_df["generated_text"] = full_df["generated_text"].fillna("").astype(str)
        
        # sélection et réorganisation des colonnes
        cols_wanted = ["app_id", "title", "rating", "method", "generated_text"]
        cols_present = [c for c in cols_wanted if c in full_df.columns]
        full_df = full_df[cols_present]
        
        # création du dossier parent si nécessaire et sauvegarde
        OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
        full_df.to_csv(OUT_PATH, index=False, encoding="utf-8")
        print(f"\n Succès ! {len(full_df)} reviews fusionnées vers {OUT_PATH}")
    else:
        print("\n Aucun fichier fusionné.")

if __name__ == "__main__":
    main()

================================================================================
FILE: src/genai/07_evaluate.py
================================================================================

import sys
from pathlib import Path
import argparse
import pandas as pd
import joblib
import csv
import numpy as np

sys.path.append(str(Path(__file__).resolve().parents[2]))


from src.config import MODELS_DIR, EVAL_DIR, GENAI_INPUTS_DIR

from src.utils import SBERTEmbedder

MIN_WORDS = 100
MAX_WORDS = 140
POS_RATING_THRESHOLD = 7

def count_words(text: str) -> int:
    return len(str(text).strip().split())

def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input",
        type=str,
        default=str(GENAI_INPUTS_DIR / "prompt_batch_filled.csv"),
        help="Input CSV containing prompts and generated_text.",
    )
    parser.add_argument(
        "--prefix",
        type=str,
        default="generation_eval",
        help="Prefix for output files in reports/results/.",
    )
    
    parser.add_argument(
        "--model",
        type=str,
        choices=["sbert", "tfidf"],
        default="sbert",
        help="Which judge model to use? (Default: sbert)"
    )
    args = parser.parse_args()

    # construction du chemin du modèle
    model_filename = f"judge_model_{args.model}.joblib"
    model_path = MODELS_DIR / model_filename

    input_path = Path(args.input)
    if not input_path.exists():
        raise FileNotFoundError(f"Missing input file: {input_path}")

    if not model_path.exists():
        raise FileNotFoundError(
            f"Missing judge model: {model_path}. Run 'python src/judge/03_train_judge.py --model {args.model}' first."
        )

    # chargement des données
    df = pd.read_csv(input_path, encoding="utf-8", engine="python")
    
    required_cols = {"rating", "method", "generated_text"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"Input CSV missing columns: {sorted(missing)}")

    df["generated_text"] = df["generated_text"].fillna("").astype(str)
    used = df[df["generated_text"].str.strip().str.len() > 0].copy()

    if used.empty:
        raise ValueError("No filled generations found. Fill generated_text for some rows first.")

    # chargement du modèle juge
    print(f"Loading judge model ({args.model.upper()}) from {model_path}...")
    model = joblib.load(model_path)

    # prédiction
    # le pipeline gère lui-même la transformation (sbert ou tf-idf)
    try:
        # cas probabiliste (rare avec linearsvc sauf si calibré)
        proba_pos = model.predict_proba(used["generated_text"])[:, 1]
        pred_label = (proba_pos >= 0.5).astype(int)
    except AttributeError:
        # cas linearsvc standard (fonction de décision)
        scores = model.decision_function(used["generated_text"])
        proba_pos = 1 / (1 + np.exp(-scores)) # sigmoïde pour simuler une probabilité
        pred_label = (scores >= 0).astype(int)

    used["pred_proba_recommended"] = proba_pos
    used["pred_label"] = pred_label

    # calcul des métriques
    used["target_label"] = (used["rating"].astype(float) >= POS_RATING_THRESHOLD).astype(int)
    used["compliant"] = (used["pred_label"] == used["target_label"]).astype(int)
    used["word_count"] = used["generated_text"].apply(count_words)
    used["length_ok_100_140"] = (
        (used["word_count"] >= MIN_WORDS) & (used["word_count"] <= MAX_WORDS)
    ).astype(int)

    # sauvegarde
    # on ajoute le nom du modèle au préfixe pour ne pas écraser les résultats
    final_prefix = f"{args.prefix}_{args.model}"
    
    out_rows_path = EVAL_DIR / f"{final_prefix}_rows.csv"
    out_summary_path = EVAL_DIR / f"{final_prefix}_summary.csv"
    out_txt_path = EVAL_DIR / f"{final_prefix}_report.txt"

    out_rows_path.parent.mkdir(parents=True, exist_ok=True)
    used.to_csv(out_rows_path, index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)

    summary = (
        used.groupby("method", dropna=False)
        .agg(
            n=("generated_text", "size"),
            compliance_rate=("compliant", "mean"),
            mean_pred_proba=("pred_proba_recommended", "mean"),
            length_ok_rate=("length_ok_100_140", "mean"),
            mean_word_count=("word_count", "mean"),
        )
        .reset_index()
        .sort_values("n", ascending=False)
    )
    summary.to_csv(out_summary_path, index=False, encoding="utf-8", quoting=csv.QUOTE_ALL)

    summary_str = summary.to_string(index=False)
    
    with open(out_txt_path, "w", encoding="utf-8") as f:
        f.write("=== GENERATION EVALUATION REPORT ===\n\n")
        f.write(f"Model Type:  {args.model.upper()}\n")
        f.write(f"Input File:  {input_path}\n")
        f.write(f"Model Path:  {model_path}\n")
        f.write("-" * 40 + "\n\n")
        f.write(summary_str)
        f.write("\n\n" + "-" * 40 + "\n")
        f.write("Legend:\n")
        f.write("- compliance_rate: % of reviews matching the target sentiment (Pos/Neg)\n")
    
    print("\n" + "="*40)
    print(f"      RÉSULTATS ({args.model.upper()})")
    print("="*40)
    print(summary_str)
    print("="*40)
    print(f"\nFichiers générés dans {EVAL_DIR} :")
    print(f"1. {out_rows_path.name}")
    print(f"2. {out_summary_path.name}")
    print(f"3. {out_txt_path.name}")


if __name__ == "__main__":
    main()

================================================================================
FILE: src/genai/prompts.md
================================================================================

# Prompt experiments – Steam review generation

## Prompt 1 – Naive prompt (positive)

**Prompt:**
Write a positive Steam review for the game Elden Ring.

**Generated review:**

Elden Ring is a masterpiece that rewards curiosity, patience, and courage.
From the moment you step into the Lands Between, the sense of freedom is unmatched. The open world isn’t just big—it’s meaningful. Every ruin on the horizon, every strange landmark, every side path feels like an invitation to discover something unforgettable. Exploration is constantly rewarded, whether with hidden bosses, breathtaking vistas, or lore fragments that slowly piece together a rich and mysterious world.
The combat is deep, demanding, and incredibly satisfying. It builds on FromSoftware’s signature Souls-style gameplay while giving players more flexibility than ever—stealth, magic, summons, mounted combat—there’s room to craft your own playstyle. Boss fights are intense and memorable, striking a perfect balance between challenge and fairness.
Artistically, the game is stunning. The environments, enemy designs, and music create an atmosphere that’s both epic and haunting. The lore, shaped with George R. R. Martin’s influence, is subtle yet compelling, encouraging you to engage with the world beyond simple objectives.
Elden Ring doesn’t hold your hand—and that’s exactly why it’s so rewarding. It trusts the player, respects their intelligence, and delivers one of the most immersive RPG experiences in years.
An absolute must-play for anyone who loves exploration, challenge, and truly great game design. ⭐⭐⭐⭐⭐

**Comment:**
- Very fluent and coherent
- Clearly positive
- Feels generic and overly polished
- Lacks concrete user complaints or imperfections

## Prompt 2 – Engineered prompt (positive)

**Prompt:**
You are a regular Steam user.

Write a positive review for the game "Elden Ring".

Constraints:
- Length: 100 to 140 words
- Mention exactly 2 positive points and 1 negative point
- Casual, honest tone (not professional or marketing-like)
- No spoilers
- Output only the review text

**Generated review:**

Elden Ring hooked me way more than I expected. The first big positive is the sense of exploration: the world feels huge and mysterious, and wandering off the main path constantly leads to cool surprises without the game holding your hand. The second positive is the combat, which is tough but fair, letting you experiment with different builds and playstyles until something finally clicks and feels incredibly rewarding.

That said, it’s not perfect. My main negative is the technical performance on PC, with occasional stutters that can break immersion, especially during intense moments.

Overall, despite that issue, the game kept pulling me back for “just one more try,” and that feeling alone made the experience totally worth it.

**Comment:**
- More realistic and user-like
- Includes a concrete negative point
- Less generic than the naive prompt
- Clearly structured while remaining natural

## Prompt 3 – Engineered prompt (negative)

**Prompt:**
You are a regular Steam user.

Write a negative review for the game "Elden Ring".

Constraints:
- Length: 100 to 140 words
- Mention exactly 2 negative points and 1 positive point
- Casual, honest tone
- No spoilers
- Output only the review text

**Generated review:**
I really wanted to love this game, but it didn’t fully click for me. The world itself is impressive and atmospheric, and exploring it can feel genuinely rewarding when you stumble onto something cool on your own — that sense of scale is the one thing I consistently enjoyed.

That said, the difficulty balance often feels more frustrating than satisfying. A lot of encounters seem tuned to punish experimentation, which can make progress feel like banging your head against a wall instead of learning naturally. On top of that, the PC performance is rough at times, with stutters and frame drops that seriously hurt immersion during fights.

Overall, there’s something special here, but the frustration and technical issues kept me from having as much fun as I hoped.

**Comment:**
- Clearly negative overall sentiment
- Includes a credible positive aspect, making the review more realistic
- Tone and structure strongly resemble real Steam user reviews
- Demonstrates that prompt constraints guide content without changing the model




================================================================================
FILE: src/data/01_collect.py
================================================================================

import sys
from pathlib import Path
import time

# Ajout de la racine du projet au path pour pouvoir importer src.config
sys.path.append(str(Path(__file__).resolve().parents[2]))

import argparse
import pandas as pd
import requests
from tqdm import tqdm
from src.config import TRAIN_APP_IDS, VALIDATION_APP_IDS, FILES

# Limites pour le scraping
REVIEWS_PER_GAME = 2000
REQUEST_SLEEP = 1.0

def fetch_reviews(app_ids, output_path):
    """Récupère les reviews via l'API Steam et sauvegarde en CSV."""
    print(f"--- Collecte des reviews pour {len(app_ids)} jeux ---")
    rows = []

    for app_id in tqdm(app_ids, desc="Downloading Reviews"):
        cursor = "*"
        collected = 0
        
        # On boucle tant qu'on n'a pas le quota de reviews pour ce jeu
        while collected < REVIEWS_PER_GAME:
            try:
                # Pagination API Steam via le paramètre 'cursor'
                url = f"https://store.steampowered.com/appreviews/{app_id}"
                params = {"json": 1, "filter": "recent", "language": "english", "num_per_page": 100, "cursor": cursor}
                resp = requests.get(url, params=params, timeout=10)
                data = resp.json()
                
                reviews = data.get("reviews", [])
                if not reviews: break
                
                # Mise à jour du curseur pour la page suivante
                cursor = data.get("cursor", cursor)

                for r in reviews:
                    rows.append({
                        "app_id": app_id,
                        "recommended": r.get("voted_up"),
                        "review_text": r.get("review", ""),
                        "votes_up": r.get("votes_up")
                    })
                    collected += 1
                    if collected >= REVIEWS_PER_GAME: break
                
                # Pause pour éviter le rate-limiting
                time.sleep(REQUEST_SLEEP)

            except Exception as e:
                print(f"Error on app {app_id}: {e}")
                break
    
    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False)
    print(f"Sauvegardé : {output_path} ({len(df)} lignes)")

def fetch_titles(app_ids, output_path):
    """Récupère le nom des jeux pour faire la correspondance ID -> Titre."""
    print(f"--- Récupération des titres pour {len(app_ids)} jeux ---")
    data = []

    # Utilisation d'une session pour garder la connexion ouverte
    with requests.Session() as s:
        for app_id in tqdm(app_ids, desc="Fetching Titles"):
            try:
                resp = s.get("https://store.steampowered.com/api/appdetails", params={"appids": app_id}, timeout=10)
                if resp.status_code == 200:
                    json_data = resp.json()
                    # L'API appdetails renvoie une structure {id: {success: bool, data: {...}}}
                    if json_data[str(app_id)]["success"]:
                        data.append({"app_id": app_id, "title": json_data[str(app_id)]["data"]["name"]})
            except Exception:
                pass
            time.sleep(0.5)
    
    df = pd.DataFrame(data)
    df.to_csv(output_path, index=False)
    print(f"Sauvegardé : {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("mode", choices=["train", "validation"], help="Quel dataset construire ?")
    args = parser.parse_args()

    # Chargement de la config selon le mode choisi
    target_ids = TRAIN_APP_IDS if args.mode == "train" else VALIDATION_APP_IDS
    paths = FILES[args.mode]

    fetch_reviews(target_ids, paths["raw_reviews"])
    fetch_titles(target_ids, paths["titles"])

================================================================================
FILE: src/data/02_process.py
================================================================================

import sys
from pathlib import Path

# Ajout du chemin racine pour trouver src.config
sys.path.append(str(Path(__file__).resolve().parents[2]))

import argparse
import pandas as pd
from src.config import FILES

def main(mode):
    paths = FILES[mode]
    print(f"--- Construction du dataset : {mode.upper()} ---")

    # Vérification de l'existence des fichiers générés à l'étape précédente
    if not paths["raw_reviews"].exists() or not paths["titles"].exists():
        print("Fichiers manquants. Lancez d'abord 01_collect_data.py")
        return

    # 1. Chargement et fusion
    reviews = pd.read_csv(paths["raw_reviews"])
    titles = pd.read_csv(paths["titles"])
    
    # Association du titre du jeu à chaque review
    df = reviews.merge(titles, on="app_id", how="left")
    
    # 2. Nettoyage
    initial_len = len(df)
    # Gestion des valeurs nulles et conversion en chaîne de caractères
    df["review_text"] = df["review_text"].fillna("").astype(str)
    
    # Suppression des reviews trop courtes (< 30 caractères)
    df = df[df["review_text"].str.len() >= 30] 
    
    # Suppression des doublons (même jeu, même texte)
    df = df.drop_duplicates(subset=["app_id", "review_text"]) 
    
    print(f"Nettoyage : {initial_len} -> {len(df)} reviews.")

    # 3. Formatage des labels
    # Conversion du vote binaire : Recommandé (1) -> 9, Non recommandé (0) -> 3
    df["rating"] = df["recommended"].astype(int).map({1: 9, 0: 3})
    
    # Sauvegarde
    cols = ["app_id", "title", "rating", "review_text"] # Sélection des colonnes pertinentes
    df[cols].to_csv(paths["final"], index=False)
    print(f"Dataset final généré : {paths['final']}")
    print(df.head(3))

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("mode", choices=["train", "validation"])
    args = parser.parse_args()
    main(args.mode)
